# 数据获取

- **集搜客：**能够直接获得处理、清洗好的数据；
- **Python**：能够使用爬虫获得足量的数据。

## 如何使用Python获取数据

- 需要掌握HTML网页的解析，识别<div>元素；
- 使用浏览器开发者工具，识别各个Element，使用指针工具对应网页和代码，可以展示元素名、属性、尺寸；
- 使用Python的第三方库，可以是“Requests”也可以是“bs4”，都是能够抓包的Package。

## 解析数据：如何伪装

- 如果返回值是“418”，说明遇到了反爬虫程序；
- 确认“Preserve log”选项勾选，点击“Network”选项可，找到“headers”，选择“User-Agent”这一项；
- 在发送请求的时候，设置“headers”为上一步得到的结果，这样就可以伪装成浏览器在访问。

# 数据预处理

## 从哪些工作入手？

1. 分词：英文不需要分词，但是中文需要；
2. 信息清理与合并（停用词）：删除“的”、“了”、“是”等停用词，通常可以建立一个停用词表；
3. 数据标注（可以标注词性）。

## 基于词典的分词方法

1. **正向最大匹配：**认为词的颗粒度越大，能够表达的意思越准确；
2. **逆向最大匹配：**“正向”最大的反面；
3. **双向最大匹配：**同时进行“正向”和“反向”两种匹配，如果结果不同，那就取更小的集合，把词语再细分；
4. **最少词数分词：**每次都切分最大的词，然后在剩余的句子当中切分最大的，不断循环，直到所有的子集都在字典里。

基于词典的方法很难解决“新词”和“歧义”的问题，词典也无法实时更新。

## 基于统计的分词方法

1. 隐马尔可夫模型（Hidden Markov Model）
2. 条件随机场（Conditional Random     Fiedl，CRF）
3. 基于深度学习的方法

HMM和CRF方法，都是将每个词进行归类：词首（B）、词中（M）、词尾（E）、单字成词，预测时会针对每个字生成不同的词位。HMM是生成式模型，CRF是判别式模型。

## 结合词典和统计：jieba分词



 